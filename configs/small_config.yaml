# Small configuration for Text-to-CAD model (testing)

# Model Architecture
model:
  vocab_size: 1000
  text_encoder_name: "bert-base-uncased"
  d_model: 128
  nhead: 4
  num_decoder_layers: 2
  dim_feedforward: 256
  dropout: 0.1
  max_seq_length: 64
  max_grad_norm: 1.0

# Training
training:
  batch_size: 8
  num_epochs: 10
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 100
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  early_stopping_patience: 3
  save_steps: 100
  eval_steps: 50
  logging_steps: 10

# Generation
generation:
  max_length: 64
  temperature: 0.7
  top_k: 50
  top_p: 0.95
  num_return_sequences: 1
  early_stopping: true

# Validation
validation:
  min_thickness: 0.5
  check_topology: true
  check_self_intersection: true
  check_watertightness: true
  mesh_quality: 0.1

# Data
data:
  train_file: "data/processed/train_small.json"
  val_file: "data/processed/val_small.json"
  test_file: "data/processed/test_small.json"
  max_text_length: 32
  max_cad_length: 64
  num_workers: 2
  pin_memory: true

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/training_small.log"
  wandb:
    project: "text-to-cad"
    entity: null
    tags: ["small", "test"]

# Paths
paths:
  output_dir: "outputs/small"
  checkpoint_dir: "checkpoints/small"
  log_dir: "logs"
  temp_dir: "temp"

# Hardware
hardware:
  device: "cpu"  # Use CPU for testing
  num_gpus: 0
  mixed_precision: false
  deterministic: true

# Deployment
deployment:
  host: "localhost"
  port: 8000
  workers: 1
  timeout: 30
  max_batch_size: 8
  model_path: "checkpoints/small/final_model.pt"