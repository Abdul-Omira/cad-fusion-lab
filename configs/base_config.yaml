# Base configuration for Text-to-CAD model

# Model Architecture
model:
  vocab_size: 10000
  text_encoder_name: "bert-base-uncased"
  d_model: 512
  nhead: 8
  num_decoder_layers: 24
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_length: 512
  max_grad_norm: 1.0

# Training
training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  early_stopping_patience: 5
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100

# Generation
generation:
  max_length: 512
  temperature: 0.7
  top_k: 50
  top_p: 0.95
  num_return_sequences: 1
  early_stopping: true

# Validation
validation:
  min_thickness: 0.5
  check_topology: true
  check_self_intersection: true
  check_watertightness: true
  mesh_quality: 0.1

# Data
data:
  train_file: "data/processed/train.json"
  val_file: "data/processed/val.json"
  test_file: "data/processed/test.json"
  max_text_length: 128
  max_cad_length: 512
  num_workers: 4
  pin_memory: true

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/training.log"
  wandb:
    project: "text-to-cad"
    entity: null
    tags: ["base"]

# Paths
paths:
  output_dir: "outputs"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  temp_dir: "temp"

# Hardware
hardware:
  device: "cuda"  # or "cpu"
  num_gpus: 1
  mixed_precision: true
  deterministic: false

# Deployment
deployment:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  timeout: 60
  max_batch_size: 32
  model_path: "checkpoints/final_model.pt"