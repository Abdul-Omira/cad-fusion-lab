# Large configuration for Text-to-CAD model (high-performance)

# Model Architecture
model:
  vocab_size: 20000
  text_encoder_name: "bert-large-uncased"
  d_model: 1024
  nhead: 16
  num_decoder_layers: 48
  dim_feedforward: 4096
  dropout: 0.1
  max_seq_length: 1024
  max_grad_norm: 1.0

# Training
training:
  batch_size: 64
  num_epochs: 200
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 2000
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  early_stopping_patience: 10
  save_steps: 2000
  eval_steps: 1000
  logging_steps: 100

# Generation
generation:
  max_length: 1024
  temperature: 0.7
  top_k: 100
  top_p: 0.95
  num_return_sequences: 3
  early_stopping: true

# Validation
validation:
  min_thickness: 0.5
  check_topology: true
  check_self_intersection: true
  check_watertightness: true
  mesh_quality: 0.05  # Higher quality mesh for better validation

# Data
data:
  train_file: "data/processed/train_large.json"
  val_file: "data/processed/val_large.json"
  test_file: "data/processed/test_large.json"
  max_text_length: 256
  max_cad_length: 1024
  num_workers: 8
  pin_memory: true

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/training_large.log"
  wandb:
    project: "text-to-cad"
    entity: null
    tags: ["large", "high-performance"]

# Paths
paths:
  output_dir: "outputs/large"
  checkpoint_dir: "checkpoints/large"
  log_dir: "logs"
  temp_dir: "temp"

# Hardware
hardware:
  device: "cuda"
  num_gpus: 4  # Multi-GPU training
  mixed_precision: true
  deterministic: false

# Deployment
deployment:
  host: "0.0.0.0"
  port: 8000
  workers: 8
  timeout: 120
  max_batch_size: 64
  model_path: "checkpoints/large/final_model.pt"